import os
from dotenv import load_dotenv
from sentence_transformers import SentenceTransformer
from pinecone import Pinecone
from groq import Groq

# ‚úÖ Load environment variables from .env
load_dotenv()

# üîó Load embedding model
print("üîó Loading embedding model...")
model = SentenceTransformer("all-MiniLM-L6-v2")

# üå≤ Connect to Pinecone
print("üå≤ Connecting to Pinecone...")
pc = Pinecone(api_key=os.environ["PINECONE_API_KEY"])
index = pc.Index("sai-rag-chatbot-index")

# ü§ñ Initialize Groq API
groq = Groq(api_key=os.environ["GROQ_API_KEY"])

# üß† Query loop
while True:
    query = input("\nüß† Ask me anything (type 'exit' to quit):\n> ")
    if query.lower() == "exit":
        break

    # 1Ô∏è‚É£ Embed the user query
    embedded_query = model.encode(query).tolist()

    # 2Ô∏è‚É£ Query Pinecone index
    results = index.query(vector=embedded_query, top_k=3, include_metadata=True)

    # 3Ô∏è‚É£ Extract context
    context = ""
    for match in results["matches"]:
        metadata = match.get("metadata", {})
        content = metadata.get("text") or metadata.get("content") or ""
        context += content.strip() + "\n---\n"

    print("\nüìö Retrieved Context:\n", context.strip())

    if not context.strip():
        print("\nü§ñ Answer:\n Sorry, I couldn't find anything relevant in the context.\n")
        continue

    # 4Ô∏è‚É£ Send to LLM via Groq
    try:
        response = groq.chat.completions.create(
            model="llama3-8b-8192",  # ‚úÖ Updated supported model
            messages=[
                {"role": "system", "content": "Answer the question using the given context below. If the answer isn't found in the context, say so."},
                {"role": "user", "content": f"Context:\n{context}\n\nQuestion:\n{query}"}
            ]
        )
        answer = response.choices[0].message.content.strip()
        print("\nü§ñ Answer:\n", answer, "\n")

    except Exception as e:
        print("\n‚ùå Groq API error:", str(e))
        print("\nü§ñ Answer:\n Sorry, something went wrong while querying the LLM.\n")
